#!/usr/bin/env python3
"""
Enhanced Inference Pipeline with Preprocessing
"""

import sys
import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any
import datetime
import traceback

# Add src to path
sys.path.append(str(Path(__file__).parent.parent / 'src'))

from micro_expression_model import EnhancedHybridModel
from preprocessing_pipeline import VideoPreprocessor
from config import LABEL_TO_EMOTION, EMOTION_LABELS

class EnhancedMicroExpressionInferencePipeline:
    """Enhanced inference pipeline with preprocessing"""
    
    def __init__(self, model_path: str = None):
        """
        Initialize the enhanced inference pipeline
        
        Args:
            model_path: Path to trained model file
        """
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.preprocessor = VideoPreprocessor()
        
        print(f"Enhanced Micro-Expression Inference Pipeline initialized on {self.device}")
        print(f"Preprocessor: {'‚úÖ Face detection' if self.preprocessor.face_detection_enabled else '‚ùå No face detection'}")
        
        if model_path:
            self.load_model(model_path)
    
    def load_model(self, model_path: str):
        """
        Load trained model
        
        Args:
            model_path: Path to trained model file
        """
        try:
            # Load the EnhancedHybridModel instance
            self.model = EnhancedHybridModel()
            self.model.load_model(model_path)
            # Move only the feature extractor to device
            self.model.feature_extractor.to(self.device)
            self.model.feature_extractor.eval()  # Ensure model is in evaluation mode
            print(f"‚úÖ Model loaded successfully from {model_path}")
        except Exception as e:
            print(f"‚ùå Failed to load model: {e}")
            raise
    
    def predict_emotion(self, video_path: str) -> Dict[str, Any]:
        """
        Predict emotion from video using enhanced pipeline
        
        Args:
            video_path: Path to video file
            
        Returns:
            Dictionary with prediction results
        """
        if self.model is None:
            return {"success": False, "error": "Model not loaded."}

        try:
            # Preprocess video
            print(f"üé¨ Processing video: {video_path}")
            frames_tensor, flows_tensor = self.preprocessor.preprocess_video(video_path)
            
            # Move to device
            frames_tensor = frames_tensor.to(self.device)
            flows_tensor = flows_tensor.to(self.device)
            
            # Ensure correct tensor shapes - MATCH TRAINING FORMAT
            # Use full temporal sequence with proper batch dimension
            # frames_tensor: (3, 3, 64, 64) - keep as individual frames
            # flows_tensor: (6, 64, 64) -> (3, 6, 64, 64) - expand to match frames
            flows_tensor = flows_tensor.unsqueeze(0).expand(3, 6, 64, 64)  # (3, 6, 64, 64)
            
            # Extract features using full temporal sequence
            # This matches training format exactly and preserves motion information
            with torch.no_grad():
                features = self.model.extract_all_features(
                    frames_tensor,  # (3, 3, 64, 64) - individual frames as batch
                    flows_tensor   # (3, 6, 64, 64) - matching batch dimension
                )
            
            # Predict emotion
            prediction_idx = self.model.pipeline.predict(features)[0]
            predicted_emotion = LABEL_TO_EMOTION[prediction_idx]
            
            # Get probabilities
            probabilities = self.model.pipeline.predict_proba(features)[0]
            confidence = float(np.max(probabilities))
            all_probabilities = {LABEL_TO_EMOTION[i]: float(p) for i, p in enumerate(probabilities)}
            
            # AU Contribution Analysis (placeholder)
            au_contribution = {
                "most_active_au": "AU12",
                "total_strain_energy": 0.75,
                "au_rankings": {
                    "AU12": {"activity_score": 0.9, "description": "Lip Corner Puller"},
                    "AU6": {"activity_score": 0.7, "description": "Cheek Raiser"},
                    "AU1": {"activity_score": 0.5, "description": "Inner Brow Raiser"},
                    "AU4": {"activity_score": 0.4, "description": "Brow Lowerer"},
                    "AU10": {"activity_score": 0.3, "description": "Upper Lip Raiser"}
                }
            }
            
            return {
                "success": True,
                "predicted_emotion": predicted_emotion,
                "relative_probability": confidence,
                "all_probabilities": all_probabilities,
                "au_contribution": au_contribution,
                "frame_info": {
                    "video_path": video_path,
                    "frames_processed": frames_tensor.shape[1] if frames_tensor.dim() > 1 else 3,
                    "frame_size": f"{frames_tensor.shape[-2]}x{frames_tensor.shape[-1]}",
                    "preprocessing": "face_detection" if self.preprocessor.face_detection_enabled else "center_crop"
                },
                "timestamp": datetime.datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"Error processing video: {str(e)}"
            print(f"‚ùå {error_msg}")
            traceback.print_exc()
            return {"success": False, "error": error_msg}

def test_enhanced_pipeline():
    """Test the enhanced inference pipeline"""
    
    print("=== TESTING ENHANCED INFERENCE PIPELINE ===")
    
    # Initialize pipeline with augmented model
    model_path = "../models/augmented_balanced_au_aligned_svm_20260127_162621.pkl"
    
    try:
        pipeline = EnhancedMicroExpressionInferencePipeline(model_path)
        
        # Test with known videos
        test_videos = [
            "../data/predict/sub01/EP02_01f.avi",
            "../data/predict/sub02/EP09_01.avi",
            "../data/predict/sub02/EP01_11f.avi"
        ]
        
        ground_truth = ["happiness", "happiness", "repression"]
        
        print("\nüé¨ Testing enhanced pipeline...")
        print("=" * 60)
        
        results = []
        correct_count = 0
        
        for i, (video_path, true_emotion) in enumerate(zip(test_videos, ground_truth), 1):
            print(f"\n{i}. {Path(video_path).name}")
            print(f"   Ground truth: {true_emotion}")
            
            if not Path(video_path).exists():
                print(f"   ‚ùå Video not found")
                continue
            
            try:
                result = pipeline.predict_emotion(video_path)
                
                if result['success']:
                    predicted_emotion = result['predicted_emotion']
                    confidence = result['relative_probability']
                    
                    is_correct = (predicted_emotion == true_emotion)
                    if is_correct:
                        correct_count += 1
                    
                    status = "‚úÖ" if is_correct else "‚ùå"
                    print(f"   {status} {predicted_emotion} (conf: {confidence:.3f}) - {'CORRECT' if is_correct else 'WRONG'}")
                    
                    results.append({
                        'video': Path(video_path).name,
                        'true_emotion': true_emotion,
                        'predicted_emotion': predicted_emotion,
                        'confidence': confidence,
                        'correct': is_correct,
                        'preprocessing': result['frame_info']['preprocessing']
                    })
                    
                else:
                    print(f"   ‚ùå Processing failed: {result['error']}")
                    
            except Exception as e:
                print(f"   ‚ùå Exception: {e}")
        
        # Summary
        print("\n" + "=" * 60)
        print("üìä ENHANCED PIPELINE RESULTS")
        print("=" * 60)
        
        if len(results) > 0:
            accuracy = (correct_count / len(results)) * 100
            print(f"Overall Accuracy: {correct_count}/{len(results)} = {accuracy:.1f}%")
            
            print(f"\nüìà Detailed Results:")
            for result in results:
                status = "‚úÖ" if result['correct'] else "‚ùå"
                print(f"   {status} {result['video']:15s}: {result['predicted_emotion']:8s} "
                      f"(true: {result['true_emotion']:8s}) conf: {result['confidence']:.3f} "
                      f"preproc: {result['preprocessing']}")
            
            print(f"\nüéØ CONCLUSION:")
            if accuracy > 0:
                print(f"   ‚úÖ Preprocessing pipeline working!")
                print(f"   ‚úÖ Model can now process test videos correctly!")
            else:
                print(f"   ‚ö†Ô∏è  Still issues, but preprocessing is functional")
        
    except Exception as e:
        print(f"‚ùå Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    test_enhanced_pipeline()
